Egyszerű Feed Forward neurális háló létrehozása lehetséges ezzel a modullal.
A háló lényegében a Brain osztály példánya, mely a <layers> lista típusú
attribútumban tárolja a neuronjait.

 Az egyes neuronok a Neuron osztály példányai. Legfontosabb attribútumaik a
<weights> és az <outputs>. Egy neuron az előtte lévő layer minden neuronjának
outputját megkapja inputként, a <weights> tartalmazza rendezett sorrendben az
ezekhez az inputokhoz tartozó súlyszámokat. Minden neuron 1 outputot képez az
összes input súlyozott összegéből az aktivációs függvény segítségével.
Az aktivációs függvény lehet:
sigmoid(x) = 1.0/(1.0+e^(-x)), ahol e az Euler-féle szám.
arctan(x)
stb.

 A háló outputja mindig egy annyi elemű vektor, ahány output neuronja van a
hálónak.
 A tanító algoritmus itt egyszerű hibaszámítás (target - output), melyet az
aktivációs függvény deriváltjával korrigálunk, így
error = aktiváció'(output) * (target - output)
A kiszámolt error alapján módosítjuk az output neuronok súlyszámait:
súly* = súly + szum(error * output!), ahol output! az egyes bemeneti neuronok
outputja (azaz az adott neuron inputjai, de ezek nincsenek külön változóban ne-
vesítve.

 A rejtett neuronok errorjának kiszámítása nem lehetséges (nincs rájuk vonat-
kozó target, csak a teljes agy kimenetére van vonatkozó targetünk) csak közvetve
a backpropagation (visszatáplálás) segítségével. Ez a következőképpen néz ki:
error = aktiváció'(output)*sum(outputot_kapó_neuron_errorja*vonatkozó_súlyszám),
ahol az <outputot kapó neuron errorja> az adott neuron outputjait felvevő
neuronok összessége.
 Az így "visszatáplált" hiba alapján korrigáljuk az adott neuron súlyszámait.

MEGJEGYZÉSEK:

- A matematikai vektor és a pythonos lista és list kifejezéseket ekvivalensként
használom, megjegyzem, helytelenül. A pythonos list alapvetően nem rendezett, de
a másik hasonló beépített adattípus, a tuple elemei nem változtathatóak.

- Ugyanebben a szellemben használom ekvivalensként az input és a stimulus kife-
jezéseket. Egy neuron/idegpálya stimulusa biológiai szempontból annak inputjával
kvázi ekvivalens.

- Az output neuronok hibaképzése és weight-korrekciója a Brain példány <learn>
metódusának keretében történik. A rejtett és input neuronoké (tehát a backpropagation)
pedig az adott Neuron példány <update_error> és <update_weights> metódusainak hivásával
(bár ezek hívását a Brain.learn() metódus végzi és lényegében inline-olhatóak lennének
oda, de olvashatóság miatt betettem a Neuronba

- Megváltoztatható (szerintem) az error számítása. Lényegében az output
vektort hasonlítjuk a target vektorhoz. Erre használhatnánk pl. euklideszi tá-
volságot, vagy "cosine similarity"-t, vagy mittudomén.

- Nem szóltam a tanulási rátáról (éta), amelyet a súlyszámok korrekciójakor
alkalmazunk:
súly' = súly + éta*(szum(error * output!))
Az általános ökölszabály kb. az, hogy minél nagyobb az éta, annál gyorsabban
tanul a háló, de annál pontatlanabb lesz utána az eredménye.

- Az osztályok szétbontása nem tökéletes, mert a Neuron önmagában életképtelen.
A metódusok priváttá tételét sem csináltam meg. Érdemes inkább az osztályokon
kívül definiált eljárásokat használni, mert nem minden osztálymetódus életképes.

- A hálózat egy egyszerűbb formája a diszkrét outputtal rendelkező perceptron
alapú háló. A perceptron egy olyan neuron, melynek az aktivációs függvénye a
küszöbfüggvény, azaz
{output = 0, ha excitáltság < küszöb; output = 1, ha excitáltság >= küszöb}
A küszöbértékből(treshold) képezhető egy "bias" érték (bias = -treshold),
mellyel a küszöbfüggvény így módosul:
{output = 0, ha excitáltság + bias < 0; output = 1, ha excitáltság + bias >= 0}
Ez a "bias" gyakorlatilag egy konstans értékkel való transzformálása az aktivá-
ciós függvénynek és tanuláskor, súly-korrekciókor a bias értékeket is
korrigálhatjuk, variálhatjuk.
A lényeg, hogy a perceptronok "bias" koncepciója megvalósítható a neuronokban
is. Én nem valósítottam meg, mert matematikailag bonyolítja a problémát,
ekkor válik szükségessé a backpropagation mellet az ún. "gradient descent"
alkalmazása, amit még nem sikerült ésszel felérnem.

- A tömörség miatt gyakran használom az ún. "list comprehension" utasításokat.
Ezek dióhéjban ezt csinálják:
[i for i in range(10)]
létrehoz egy listát: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Persze lehet trükközni:
[i*2 for i in range(10)]
létrehoz egy listát: [0, 2, 4, 6, 8, 10, ...]
Az i-re lehet függvényt is hívni, szóval elég sokoldalú tool a list comp.
Ja és a végére még mehet logikai feltétel is :D
[i for i in range(10) if i <=3]
létrehoz egy listát: [0, 1, 2, 3]
Ja és lehet ilyet is, ha lusta az ember, vagy nem akarja i-t használni:
[Neuron() for i in range(10)]
létrehoz egy listát 10 Neuron példánnyal.
Ebben a modulban használom pl. a neuronok korrigált súlyszámait tartalmazó lista
helyben történő létrehozására, valahogy így:
neuron.súlyok = [korrigáló_függvény(régi_súly) for régi_súly in neuron.súlyok]
újra létrehozza a neuron.súlyok listát a régi alapján, de korrigált súlyokkal.
